{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18298de9",
   "metadata": {},
   "source": [
    "# Topic Modeling: Organizing Unlabeled CVs with LDA\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **Topic Modeling** using **Latent Dirichlet Allocation (LDA)** to organize unlabeled CVs (resumes) by automatically discovering hidden topics. Unlike supervised classification, topic modeling works with completely unlabeled data, making it ideal for organizing large document collections without manual labeling. You'll learn how to apply LDA to discover topics, interpret results, and organize documents based on their dominant topics.\n",
    "\n",
    "> \"The best way to find a needle in a haystack is to organize the haystack first.\"\n",
    "\n",
    "**The Problem**: You have a folder full of CVsâ€”unlabeled, unorganized. You need to find candidates for specific roles, but manually reading through hundreds of CVs is impossible.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand what Topic Modeling is and why it's useful for unsupervised document organization\n",
    "- Learn how LDA (Latent Dirichlet Allocation) discovers hidden topics in text collections\n",
    "- Apply LDA to organize unlabeled documents automatically\n",
    "- Interpret topic modeling results by examining top words and document-topic distributions\n",
    "- Organize documents into folders based on their dominant topics\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Introduction to Topic Modeling** - What it is and why it's useful\n",
    "2. **What is LDA?** - Understanding Latent Dirichlet Allocation\n",
    "3. **The Pipeline** - Complete workflow from data loading to organization\n",
    "4. **Step 1: Loading Data** - Reading CVs from JSON files\n",
    "5. **Step 2: Preprocessing** - Cleaning and preparing text\n",
    "6. **Step 3: Vectorization** - Converting text to document-term matrix\n",
    "7. **Step 4: Training LDA** - Discovering topics automatically\n",
    "8. **Step 5: Analyzing Results** - Interpreting discovered topics\n",
    "9. **Step 6: Organizing Documents** - Creating folders and organizing CVs by topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded0fed",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "**Topic Modeling** is an **unsupervised learning** task that discovers hidden topics in a collection of unlabeled documents. Unlike classification (which requires labeled data), topic modeling finds patterns automatically.\n",
    "\n",
    "**Example applications:**\n",
    "- **Organizing unlabeled documents**: Group CVs by field (AI/ML, Data Analysis, etc.) without manual labeling\n",
    "- **Understanding large text collections**: Discover what themes exist in news archives, research papers, or social media\n",
    "- **Content recommendation**: Find documents similar to a given document based on topic similarity\n",
    "\n",
    "**Why it's useful:**\n",
    "- No labels needed: works with completely unlabeled data\n",
    "- Interpretable: topics are defined by their top words, making them understandable\n",
    "- Scalable: can process large document collections\n",
    "- Flexible: number of topics can be adjusted based on the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f61c3f",
   "metadata": {},
   "source": [
    "## What is LDA?\n",
    "\n",
    "**Latent Dirichlet Allocation (LDA)** is a probabilistic model that discovers hidden topics in a collection of documents.\n",
    "\n",
    "**Key idea**: \n",
    "- Each document is a **mixture of topics** (e.g., 70% AI/ML, 20% Data Analysis, 10% Software Engineering)\n",
    "- Each topic is a **distribution over words** (e.g., Topic 1: 30% \"PyTorch\", 25% \"TensorFlow\", 20% \"NLP\"...)\n",
    "- LDA discovers these topics automatically by finding words that co-occur together\n",
    "\n",
    "**For our CVs**: LDA will discover topics like \"AI/ML\", \"Data Analysis\", \"Big Data\" by looking at which words appear together, then assign each CV to the most relevant topic(s).\n",
    "\n",
    "**Reference**: Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). [Latent dirichlet allocation](https://dl.acm.org/doi/10.5555/944919.944937). *Journal of machine Learning research*, 3(Jan), 993-1022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf82c0",
   "metadata": {},
   "source": [
    "![Left: BoW. Right: LDA](../assets/lda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7018f6c",
   "metadata": {},
   "source": [
    "## The Pipeline\n",
    "\n",
    "1. **Load CVs**: Read all JSON files from topic folders using glob patterns and extract structured fields\n",
    "2. **Preprocess**: Clean the text (remove URLs, emails, etc.)\n",
    "3. **Vectorize**: Convert text to document-term matrix (Bag of Words)\n",
    "4. **Train LDA**: Discover topics automatically\n",
    "5. **Analyze Results**: See what topics were found and which CVs belong to each\n",
    "6. **Organize**: Create folders and copy CVs based on their dominant topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff53c5d",
   "metadata": {},
   "source": [
    "## Step 1: Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9332f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File > Open Folder > W5_NLP\n",
    "# (VS Code root should be at W5_NLP)\n",
    "# Then run: `uv sync`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f5dc104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c1aa196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 CV files from 0 topic folders:\n",
      "\n",
      "Combined structured data into text for 0 CVs\n"
     ]
    }
   ],
   "source": [
    "# Load CVs from JSON files in all topic folders\n",
    "cv_dir = Path('../datasets/CVs')\n",
    "# Use glob pattern to find all JSON files in Topic_* subdirectories, excluding English versions\n",
    "cv_files = sorted([f for f in cv_dir.glob('Topic_*/*.json') if not f.name.endswith('_en.json')])\n",
    "\n",
    "# Load and extract structured data from JSON\n",
    "cvs_data = []\n",
    "cv_names = []\n",
    "cv_file_paths = []  # Store original file paths for later copying\n",
    "\n",
    "for file in cv_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        cvs_data.append(data)\n",
    "        cv_names.append(file.stem)\n",
    "        cv_file_paths.append(file)  # Store the full path\n",
    "\n",
    "print(f\"Loaded {len(cvs_data)} CV files from {len(set(f.parent.name for f in cv_files))} topic folders:\")\n",
    "for i, name in enumerate(cv_names, 1):\n",
    "    print(f\"  {i}. {name}\")\n",
    "\n",
    "# Combine structured fields into text for each CV\n",
    "def combine_cv_fields(cv_json):\n",
    "    \"\"\"Combine Heading, Skills, Projects, Experience, Education into a single text\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Add heading\n",
    "    if 'Heading' in cv_json:\n",
    "        parts.append(cv_json['Heading'])\n",
    "    \n",
    "    # Add skills (join list items)\n",
    "    if 'Skills' in cv_json:\n",
    "        skills_text = ' '.join(cv_json['Skills']) if isinstance(cv_json['Skills'], list) else cv_json['Skills']\n",
    "        parts.append(skills_text)\n",
    "    \n",
    "    # Add projects\n",
    "    if 'Projects' in cv_json:\n",
    "        projects_text = ' '.join(cv_json['Projects']) if isinstance(cv_json['Projects'], list) else cv_json['Projects']\n",
    "        parts.append(projects_text)\n",
    "    \n",
    "    # Add experience\n",
    "    if 'Experience' in cv_json:\n",
    "        exp_text = ' '.join(cv_json['Experience']) if isinstance(cv_json['Experience'], list) else cv_json['Experience']\n",
    "        parts.append(exp_text)\n",
    "    \n",
    "    # Add education\n",
    "    if 'Education' in cv_json:\n",
    "        edu_text = ' '.join(cv_json['Education']) if isinstance(cv_json['Education'], list) else cv_json['Education']\n",
    "        parts.append(edu_text)\n",
    "    \n",
    "    return ' '.join(parts)\n",
    "\n",
    "# Convert JSON data to text\n",
    "cvs = [combine_cv_fields(cv_data) for cv_data in cvs_data]\n",
    "print(f\"\\nCombined structured data into text for {len(cvs)} CVs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbef1b5",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5c7bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 0 CVs\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean text: remove URLs, emails, and normalize whitespace\"\"\"\n",
    "    # Remove emails and URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Keep only Arabic/English letters and numbers\n",
    "    text = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Preprocess all CVs\n",
    "cvs_processed = [preprocess_text(cv) for cv in cvs]\n",
    "print(f\"Preprocessed {len(cvs_processed)} CVs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e1f26b",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for LDA\n",
    "\n",
    "Convert text to a document-term matrix (same as Bag of Words from classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db57b37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-empty CVs: 0\n"
     ]
    }
   ],
   "source": [
    "cvs_nonempty = [cv for cv in cvs_processed if len(cv.strip()) > 0]\n",
    "print(f\"Number of non-empty CVs: {len(cvs_nonempty)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a63cfb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in folder: ['01_en.json', '01_en.md', '05_en.json', '05_en.md', '07.json', '07.md', '10.json', '10.md', '11.json', '11.md', '15.json', '15.md', '17.json', '17.md', '20.json', '20.md', '21.json', '21.md', '22.json', '22.md', '26.json', '26.md', '29.json', '29.md', '30.json', '30.md', '33.json', '33.md', '34.json', '34.md', '39.json', '39.md', '40.json', '40.md']\n",
      "Sample data from first CV: {'Heading': 'Ahmed Al-Otaibi, AI Engineer', 'Skills': ['*', '**Programming', 'Languages:**', 'Python,', 'C++,', 'SQL. *', '**Machine', '&', 'Deep', 'Learning:**', 'PyTorch,', 'TensorFlow,', 'Scikit-learn,', 'Keras,', 'Hugging', 'Face', '(Transformers). *', '**Deployment', '&', 'Cloud', 'Computing:**', 'Docker,', 'Kubernetes,', 'AWS', '(SageMaker),', 'FastAPI,', 'CI/CD', 'Pipelines. *', '**Specializations:**', 'NLP', '(Natural', 'Language', 'Processing),', 'Computer', 'Vision,', 'Generative', 'AI.  ---'], 'Projects': ['Arabic Sentiment Analysis System | AI Model Developer [02/2023 - 05/2023]  Fine-tuned the AraBERT model on a dataset of over 100,000 tweets for dialect classification.  Achieved 94% accuracy, outperforming baseline models by 12%.', 'Defect Detection System in Manufacturing | Computer Vision Engineer [09/2022 - 12/2022]  Developed a YOLOv8 algorithm for real-time defect detection on production lines using video footage.  Reduced the false positive rate by 20%, which sped up the manual inspection process.  ---'], 'Experience': ['AI Engineer', 'FutureTech | Riyadh [06/2023] â€“ [Present]  Deployed deep learning models to a production environment using Docker and FastAPI to serve over 50,000 requests daily.  Improved API latency by 40% by optimizing the model architecture and reducing its size (Quantization).  Designed a Recommendation Engine that contributed to a 15% increase in user engagement during the first quarter.', 'Junior Machine Learning Engineer', 'DataCo | Jeddah [01/2022] â€“ [05/2023]  Automated the data collection and cleaning process using Python Scripts, saving the team 10 hours of work per week.  Participated in developing a Demand Forecasting model that helped the client reduce inventory waste by 8%.  ---'], 'Education': ['Bachelor of Computer Science', 'King Saud University | Riyadh [05/2021]  GPA: 4.75/5  Professional Certifications: Deep Learning Nanodegree (Udacity), AWS Certified Machine Learning.']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "base_folder = r\"C:\\Users\\lamia\\Downloads\\B5-main\\B5-main\\W5_NLP\\M1\\datasets\\CVs\"\n",
    "topic_folder = os.path.join(base_folder, \"Topic_1\")\n",
    "\n",
    "print(\"Files in folder:\", os.listdir(topic_folder))\n",
    "\n",
    "first_file = os.path.join(topic_folder, os.listdir(topic_folder)[0])\n",
    "with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print(\"Sample data from first CV:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88b852e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CVs extracted: 17\n",
      "Sample CV text:\n",
      " Ahmed Al-Otaibi, AI Engineer * **Programming Languages:** Python, C++, SQL. * **Machine & Deep Learning:** PyTorch, TensorFlow, Scikit-learn, Keras, Hugging Face (Transformers). * **Deployment & Cloud Computing:** Docker, Kubernetes, AWS (SageMaker), FastAPI, CI/CD Pipelines. * **Specializations:** NLP (Natural Language Processing), Computer Vision, Generative AI.  --- Arabic Sentiment Analysis System | AI Model Developer [02/2023 - 05/2023]  Fine-tuned the AraBERT model on a dataset of over 100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "base_folder = r\"C:\\Users\\lamia\\Downloads\\B5-main\\B5-main\\W5_NLP\\M1\\datasets\\CVs\"\n",
    "topic_folder = os.path.join(base_folder, \"Topic_1\")  # ØºÙŠØ±ÙŠ Ø§Ù„Ø±Ù‚Ù… Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©\n",
    "\n",
    "cvs = []\n",
    "\n",
    "for filename in os.listdir(topic_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(topic_folder, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ù†Øµ ÙˆØ§Ø­Ø¯\n",
    "            text_parts = []\n",
    "            if \"Heading\" in data: text_parts.append(data[\"Heading\"])\n",
    "            if \"Skills\" in data: text_parts.append(\" \".join(data[\"Skills\"]))\n",
    "            if \"Projects\" in data: text_parts.append(\" \".join(data[\"Projects\"]))\n",
    "            if \"Experience\" in data: text_parts.append(\" \".join(data[\"Experience\"]))\n",
    "            \n",
    "            full_text = \" \".join(text_parts).strip()\n",
    "            \n",
    "            if full_text:  # ÙÙ‚Ø· Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØºÙŠØ± ÙØ§Ø±ØºØ©\n",
    "                cvs.append(full_text)\n",
    "\n",
    "print(f\"Number of CVs extracted: {len(cvs)}\")\n",
    "print(\"Sample CV text:\\n\", cvs[0][:500])  # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 500 Ø­Ø±Ù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "277cb9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Term Matrix: 17 CVs Ã— 907 words\n",
      "Sparsity: 86.7%\n"
     ]
    }
   ],
   "source": [
    "# Create document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=1000,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    ")\n",
    "doc_term_matrix = vectorizer.fit_transform(cvs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Document-Term Matrix: {doc_term_matrix.shape[0]} CVs Ã— {doc_term_matrix.shape[1]} words\")\n",
    "print(f\"Sparsity: {(1 - doc_term_matrix.nnz / (doc_term_matrix.shape[0] * doc_term_matrix.shape[1])) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29757a7c",
   "metadata": {},
   "source": [
    "## Step 4: Train LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "042a3137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA to discover 3 topics...\n",
      "âœ“ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model\n",
    "n_topics = 3  # Number of topics to discover\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=10,\n",
    "    learning_method='online'\n",
    ")\n",
    "\n",
    "print(f\"Training LDA to discover {n_topics} topics...\")\n",
    "lda.fit(doc_term_matrix)\n",
    "print(\"âœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b719dc1",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results\n",
    "\n",
    "Let's see what topics LDA discovered and which words define each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58b72bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "  Top words: churn, a, engineer, model, to, 06, for, equipment, 2022, by\n",
      "  Weights: ['3.386', '2.105', '2.009', '1.787', '1.499', '1.481', '1.436', '1.435', '1.397', '1.382']\n",
      "\n",
      "Topic 2:\n",
      "  Top words: a, to, ai, the, and, for, 2023, on, models, engineer\n",
      "  Weights: ['55.174', '42.661', '39.798', '39.366', '38.671', '35.840', '31.134', '28.985', '27.747', '27.306']\n",
      "\n",
      "Topic 3:\n",
      "  Top words: the, engineer, model, ci, ml, mlops, infrastructure, cd, models, and\n",
      "  Weights: ['9.787', '8.283', '7.857', '6.612', '6.364', '6.357', '5.862', '5.837', '4.920', '4.908']\n"
     ]
    }
   ],
   "source": [
    "# Display top words for each topic\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    \"\"\"Display top words for each topic\"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        top_weights = [topic[i] for i in top_words_idx]\n",
    "        \n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        print(\"  Top words:\", \", \".join(top_words))\n",
    "        print(\"  Weights:\", [f\"{w:.3f}\" for w in top_weights])\n",
    "\n",
    "display_topics(lda, feature_names, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bab037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CVs in doc_term_matrix: 17\n",
      "Number of CV names: 0\n",
      "Number of dominant topics: 17\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of CVs in doc_term_matrix: {doc_term_matrix.shape[0]}\")\n",
    "print(f\"Number of CV names: {len(cv_names)}\")\n",
    "print(f\"Number of dominant topics: {len(dominant_topics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66088b1",
   "metadata": {},
   "source": [
    "**Interpreting the topics**: Look at the top words for each topic. Can you guess what each topic represents? For example:\n",
    "- Topic with \"PyTorch\", \"TensorFlow\", \"NLP\" â†’ probably AI/ML\n",
    "- Topic with \"Tableau\", \"Power BI\", \"dashboard\" â†’ probably Data Analysis\n",
    "- Topic with \"Hadoop\", \"Spark\", \"Kafka\" â†’ probably Big Data\n",
    "\n",
    "Now let's see which CV belongs to which topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f168fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Assignment to Topics:\n",
      "============================================================\n",
      "\n",
      "Topic 1 (0 CVs):\n",
      "\n",
      "Topic 2 (13 CVs):\n",
      "  - Ahmed Al-Otaibi, AI Engineer * **Programming Langu... (99.7%)\n",
      "  - Fatima Al-Zahrani, AI Engineer * **Programming Lan... (99.6%)\n",
      "  - Ali Bin Nasser, AI Engineer * **Research Areas:** ... (99.5%)\n",
      "  - Nasser Al-Khaldi, AI Engineer * **Programming Lang... (69.0%)\n",
      "  - Dana Al-Jaber, AI Engineer * **Generative Models:*... (99.6%)\n",
      "  - Abdullah Al-Ghamdi, AI Engineer * **Research Areas... (99.5%)\n",
      "  - Yasmin Al-Jassim, AI Engineer * **Programming Lang... (99.6%)\n",
      "  - Ibrahim Al-Saeed, AI Engineer * **Programming:** P... (99.6%)\n",
      "  - Hamad Al-Ghanim, Data Scientist * **Python:** Pand... (99.6%)\n",
      "  - Omar Bin Talal, AI Engineer * **Generative Models:... (99.6%)\n",
      "  - Laila Al-Qahtani, AI Engineer * **LLMs:** Fine-tun... (99.6%)\n",
      "  - Ahmed Taha, AI Engineer * **Research Focus:** Mach... (99.5%)\n",
      "  - Fatima Al-Shamsi, AI Engineer * **Research Interes... (99.5%)\n",
      "\n",
      "Topic 3 (4 CVs):\n",
      "  - Layla Al-Harbi, AI Engineer * **Automation & Pipel... (88.3%)\n",
      "  - Sultan Al-Fahim, AI Engineer * **CI/CD & Automatio... (74.4%)\n",
      "  - Faisal Al-Qahtani, AI Engineer * **Infrastructure ... (72.3%)\n",
      "  - Nouf Al-Hamad, AI Engineer * **Model Deployment:**... (69.7%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cv_preview = [cv[:50] + (\"...\" if len(cv) > 50 else \"\") for cv in cvs]\n",
    "\n",
    "# Ø¥Ù†Ø´Ø§Ø¡ DataFrame Ù„Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "df_results = pd.DataFrame({\n",
    "    'CV': cv_preview,\n",
    "    'Dominant Topic': dominant_topics + 1,\n",
    "    'Topic Probabilities': list(doc_topic_dist)\n",
    "})\n",
    "\n",
    "# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "print(\"CV Assignment to Topics:\")\n",
    "print(\"=\" * 60)\n",
    "for topic_id in range(n_topics):\n",
    "    topic_cvs = df_results[df_results['Dominant Topic'] == topic_id + 1]\n",
    "    print(f\"\\nTopic {topic_id + 1} ({len(topic_cvs)} CVs):\")\n",
    "    for idx, row in topic_cvs.iterrows():\n",
    "        prob = row['Topic Probabilities'][topic_id]\n",
    "        print(f\"  - {row['CV']} ({prob:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086c24a",
   "metadata": {},
   "source": [
    "## Step 6: Organize CVs into Folders\n",
    "\n",
    "Now comes the practical part: **automatically organize CVs into folders** based on their dominant topic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8ee314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Organization complete! CVs are now in: output\\organized_cvs\n"
     ]
    }
   ],
   "source": [
    "# Create output directory structure\n",
    "output_dir = Path('output/organized_cvs')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a folder for each topic\n",
    "for topic_id in range(n_topics):\n",
    "    topic_dir = output_dir / f\"Topic_{topic_id + 1}\"\n",
    "    topic_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy each CV to its topic folder\n",
    "for idx, (cv_name, topic_id, source_file) in enumerate(zip(cv_names, dominant_topics, cv_file_paths)):\n",
    "    target_dir = output_dir / f\"Topic_{topic_id + 1}\"\n",
    "    target_file = target_dir / f\"{cv_name}.json\"\n",
    "    \n",
    "    shutil.copy2(source_file, target_file)\n",
    "    print(f\"Copied {cv_name}.json â†’ Topic_{topic_id + 1}/\")\n",
    "\n",
    "print(f\"\\nâœ“ Organization complete! CVs are now in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5be03",
   "metadata": {},
   "source": [
    "### Verify the Organization\n",
    "\n",
    "Let's check what's in each folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "445a901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic_1/ (0 CVs):\n",
      "\n",
      "Topic_2/ (0 CVs):\n",
      "\n",
      "Topic_3/ (0 CVs):\n"
     ]
    }
   ],
   "source": [
    "# Show contents of each topic folder\n",
    "for topic_id in range(n_topics):\n",
    "    topic_dir = output_dir / f\"Topic_{topic_id + 1}\"\n",
    "    files = list(topic_dir.glob('*.json'))\n",
    "    print(f\"\\nTopic_{topic_id + 1}/ ({len(files)} CVs):\")\n",
    "    for f in sorted(files):\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48759d",
   "metadata": {},
   "source": [
    "## **Student Exercise**: discover topics on a dataset of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffffa83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahmed Al-Otaibi, AI Engineer * **Programming Langu... --> Topic 1\n",
      "Layla Al-Harbi, AI Engineer * **Automation & Pipel... --> Topic 3\n",
      "Fatima Al-Zahrani, AI Engineer * **Programming Lan... --> Topic 2\n",
      "Ali Bin Nasser, AI Engineer * **Research Areas:** ... --> Topic 2\n",
      "Nasser Al-Khaldi, AI Engineer * **Programming Lang... --> Topic 1\n",
      "Sultan Al-Fahim, AI Engineer * **CI/CD & Automatio... --> Topic 3\n",
      "Dana Al-Jaber, AI Engineer * **Generative Models:*... --> Topic 2\n",
      "Abdullah Al-Ghamdi, AI Engineer * **Research Areas... --> Topic 2\n",
      "Yasmin Al-Jassim, AI Engineer * **Programming Lang... --> Topic 1\n",
      "Ibrahim Al-Saeed, AI Engineer * **Programming:** P... --> Topic 2\n",
      "Hamad Al-Ghanim, Data Scientist * **Python:** Pand... --> Topic 2\n",
      "Faisal Al-Qahtani, AI Engineer * **Infrastructure ... --> Topic 3\n",
      "Nouf Al-Hamad, AI Engineer * **Model Deployment:**... --> Topic 3\n",
      "Omar Bin Talal, AI Engineer * **Generative Models:... --> Topic 2\n",
      "Laila Al-Qahtani, AI Engineer * **LLMs:** Fine-tun... --> Topic 2\n",
      "Ahmed Taha, AI Engineer * **Research Focus:** Mach... --> Topic 2\n",
      "Fatima Al-Shamsi, AI Engineer * **Research Interes... --> Topic 1\n"
     ]
    }
   ],
   "source": [
    "# STUDENT EXERCISE\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 3  \n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "doc_topic_dist = lda.fit_transform(doc_term_matrix)\n",
    "\n",
    "dominant_topics = doc_topic_dist.argmax(axis=1)\n",
    "\n",
    "cv_preview = [cv[:50] + (\"...\" if len(cv) > 50 else \"\") for cv in cvs]\n",
    "for i, topic in enumerate(dominant_topics):\n",
    "    print(f\"{cv_preview[i]} --> Topic {topic + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213d4f9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we accomplished**:\n",
    "1. âœ… Loaded unlabeled CVs from a folder\n",
    "2. âœ… Preprocessed the text data\n",
    "3. âœ… Created a document-term matrix\n",
    "4. âœ… Trained an LDA model to discover topics\n",
    "5. âœ… Analyzed which CVs belong to which topic\n",
    "6. âœ… **Automatically organized CVs into folders** based on discovered topics\n",
    "\n",
    "**Key Takeaways**:\n",
    "- **LDA discovers topics automatically** by finding words that co-occur together\n",
    "- **Each document is a mixture of topics** - LDA assigns probabilities\n",
    "- **Topic modeling is unsupervised** - no labels needed!\n",
    "- **Practical application**: Organize unlabeled documents automatically\n",
    "\n",
    "**Next Steps**:\n",
    "- Try different numbers of topics (`n_topics`) and see how results change\n",
    "- Experiment with preprocessing (stemming, stop words removal)\n",
    "- Use topic probabilities to handle CVs that belong to multiple topics\n",
    "- Visualize topics using tools like pyLDAvis\n",
    "\n",
    "**References**:\n",
    "- [Scikit-learn LDA documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n",
    "- [Topic modeling visualization guide](https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8790015",
   "metadata": {},
   "source": [
    "\n",
    "## Module 1 Synthesis: The Complete Pipeline\n",
    "\n",
    "Congratulations! You've completed **Module 1: Text Analysis with Statistical NLP**. Let's reflect on the journey and see how all the pieces fit together.\n",
    "\n",
    "### The Circular Learning Experience\n",
    "\n",
    "Remember the question chain we started with? Let's trace how we answered each question and built a complete NLP pipeline:\n",
    "\n",
    "1. **\"What is NLP?\"** â†’ We learned that NLP bridges computers and human language, with applications in understanding and generation.\n",
    "\n",
    "2. **\"How do we extract patterns from text?\"** â†’ We used **Regular Expressions** to find, match, and manipulate text patternsâ€”essential for preprocessing.\n",
    "\n",
    "3. **\"How do we understand our data?\"** â†’ We performed **Exploratory Data Analysis (EDA)** on corpora to assess data quality, vocabulary characteristics, and preprocessing needs.\n",
    "\n",
    "4. **\"How do we prepare text for ML?\"** â†’ We applied **Preprocessing** techniques (cleaning, normalization, tokenization, stemming) to transform raw text into clean tokens.\n",
    "\n",
    "5. **\"How do we convert text to numbers?\"** â†’ We used **Vectorization** (BoW, TF-IDF) to convert text into numerical features that ML models can process.\n",
    "\n",
    "6. **\"How do we build classifiers?\"** â†’ We built **Text Classification** models (like sentiment analysis) using vectorized features and supervised learning.\n",
    "\n",
    "7. **\"How do we search documents?\"** â†’ We implemented **Information Retrieval** systems using TF-IDF and cosine similarity to find relevant documents.\n",
    "\n",
    "8. **\"How do we discover topics?\"** â†’ We applied **Topic Modeling** (LDA) to automatically organize unlabeled documents by discovering hidden topics.\n",
    "\n",
    "### The Complete NLP Pipeline\n",
    "\n",
    "Throughout this module, you've learned to build a complete NLP pipeline:\n",
    "\n",
    "```\n",
    "Raw Text\n",
    "    â†“\n",
    "[Regex: Pattern Extraction]\n",
    "    â†“\n",
    "[Corpus & EDA: Understanding Data]\n",
    "    â†“\n",
    "[Preprocessing: Cleaning & Normalization]\n",
    "    â†“\n",
    "[Vectorization: Text â†’ Numbers]\n",
    "    â†“\n",
    "[Modeling: Classification / IR / Topic Modeling]\n",
    "    â†“\n",
    "Actionable Insights\n",
    "```\n",
    "\n",
    "### Key Skills You've Acquired\n",
    "\n",
    "By completing this module, you can now:\n",
    "\n",
    "âœ… **Build supervised ML text classification pipelines**\n",
    "- Preprocess Arabic and English text\n",
    "- Vectorize text using BoW and TF-IDF\n",
    "- Train and evaluate classifiers\n",
    "- Interpret model results\n",
    "\n",
    "âœ… **Apply keyword-based information retrieval**\n",
    "- Implement TF-IDF-based search engines\n",
    "- Measure document similarity using cosine similarity\n",
    "- Rank and retrieve relevant documents\n",
    "\n",
    "âœ… **Apply unsupervised ML for document organization**\n",
    "- Discover hidden topics using LDA\n",
    "- Organize unlabeled documents automatically\n",
    "- Interpret topic modeling results\n",
    "\n",
    "### The Foundation for What's Next\n",
    "\n",
    "This module focused on **statistical NLP**â€”traditional methods that work well for many tasks. In **Module 2**, you'll learn about **Deep Learning approaches** (embeddings, transformers) that build on these foundations to achieve even better performance.\n",
    "\n",
    "**What you learned here is still valuable:**\n",
    "- Preprocessing techniques apply to both statistical and deep learning methods\n",
    "- Understanding vectorization helps you understand embeddings\n",
    "- EDA is always the first step, regardless of the approach\n",
    "- The pipeline structure (preprocess â†’ vectorize â†’ model) remains the same\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "Before moving to Module 2, consider:\n",
    "\n",
    "1. **When would you use statistical NLP vs. deep learning?**\n",
    "   - Statistical NLP: Fast, interpretable, works with small data\n",
    "   - Deep Learning: Better accuracy, requires more data and computation\n",
    "\n",
    "2. **What preprocessing steps are most important?**\n",
    "   - Depends on your data and task, but EDA always guides the decision\n",
    "\n",
    "3. **How does TF-IDF differ from BoW?**\n",
    "   - BoW: Simple word counts\n",
    "   - TF-IDF: Weighted counts that emphasize distinctive words\n",
    "\n",
    "4. **When would you use topic modeling vs. classification?**\n",
    "   - Classification: When you have labels and want to predict categories\n",
    "   - Topic Modeling: When you have no labels and want to discover structure\n",
    "\n",
    "### The Journey Continues\n",
    "\n",
    "You've built a solid foundation in statistical NLP. The concepts you've learnedâ€”preprocessing, vectorization, classification, retrieval, and topic modelingâ€”are the building blocks for more advanced techniques.\n",
    "\n",
    "**Next Module Preview:**\n",
    "- **Module 2** introduces **Deep Learning for NLP**:\n",
    "  - Tokenization with modern tools (WordPiece, BPE)\n",
    "  - Word embeddings (Word2Vec, GloVe, contextual embeddings)\n",
    "  - Transformers and BERT\n",
    "  - Fine-tuning pre-trained models\n",
    "\n",
    "The journey from statistical NLP to deep learning is a natural progressionâ€”you'll see how embeddings generalize vectorization, how transformers improve on traditional methods, and how pre-trained models leverage the foundations you've built.\n",
    "\n",
    "---\n",
    "\n",
    "**Module 1 Complete! ðŸŽ‰**\n",
    "\n",
    "You now have the skills to work with text data using statistical methods. You understand the complete pipeline from raw text to actionable insights, and you're ready to explore the power of deep learning in Module 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

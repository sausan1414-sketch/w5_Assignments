{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGwURjeeNO5p"
      },
      "source": [
        "# Generating Your First Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-D9YGfwNJxZ"
      },
      "source": [
        "This book is for the GPU-poor! We will use models that users can run without the most expensive GPU(s) available or a big budget.\n",
        "\n",
        "A free instance of **Google Colab** will give you a **T4 GPU** with 16 GB VRAM, which is the minimum amount of VRAM that we suggest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_TDY0rq-E6H"
      },
      "source": [
        "### Open Models\n",
        "\n",
        "Open LLMs are models that share their weights and architecture with the public to use. They are still developed by specific organizations but often share their code for creating or running the model locally—with varying levels of licensing that may or may not allow commercial usage of the model.\n",
        "\n",
        "Examples:\n",
        "- Cohere’s Command R\n",
        "- the Mistral models\n",
        "- Microsoft’s Phi\n",
        "- Meta’s Llama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POqFwwsK93ar"
      },
      "source": [
        "### Get started with: Phi-3-mini\n",
        "\n",
        "The main generative model we use throughout the book is [Phi-3-mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct). Small yet performant:\n",
        "\n",
        "- 3.8 Billion parameters\n",
        "- 8 GB of VRAM; 6 GB with Quantization\n",
        "\n",
        "Moreover: MIT license. Which allows the model to be used for commercial purposes without constraints!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04ab612b38be4f68848cc3dcfb35f84a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3577603430e744cabb6e70824b529db0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- configuration_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59a3efe666dd46e49060bbbdc0f80eb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- modeling_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abc3e6f51a6148088c16241698a6971c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45f4a3d3bf554544980085f45eacdaca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7b737aefb5f49f09eed4f1865d431ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "923d7fd801354764b126e0285b2c15c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "231400c76e294a169fbe8f7cb0f8ce42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd0f0a0fd90d4ae49099aea049812b33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3dc26142edc049b9b43128db3e06bf3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1630001cb354d2e83c2571850af7d6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06948a6f076a418f89b2976081b73cb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb8037b074ba41f8a81a12a0184b0df0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b10541ff8e36440bac8bf94500d23ac7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we load the `model` and `tokenizer` separately and keep them as such so that we can explore them separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|> Subject: Sincere Apologies for the Gardening Mishap\n",
            "\n",
            "\n",
            "Dear\n"
          ]
        }
      ],
      "source": [
        "# Notice the `<|assistant|>` special token at the end of the prompt\n",
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate the text\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=20\n",
        ")\n",
        "\n",
        "# Print the output\n",
        "print(tokenizer.decode(generation_output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
            "           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n",
            "           372,  9559, 29889, 32001]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write\n",
            "an\n",
            "email\n",
            "apolog\n",
            "izing\n",
            "to\n",
            "Sarah\n",
            "for\n",
            "the\n",
            "trag\n",
            "ic\n",
            "garden\n",
            "ing\n",
            "m\n",
            "ish\n",
            "ap\n",
            ".\n",
            "Exp\n",
            "lain\n",
            "how\n",
            "it\n",
            "happened\n",
            ".\n",
            "<|assistant|>\n"
          ]
        }
      ],
      "source": [
        "for id in input_ids[0]:\n",
        "   print(tokenizer.decode(id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the output is `token_ids`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
              "           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n",
              "           372,  9559, 29889, 32001,  3323,   622, 29901,   317,  3742,   406,\n",
              "          6225, 11763,   363,   278, 19906,   292,   341,   728,   481,    13,\n",
              "            13,    13, 29928,   799]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generation_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We `decode` them to get the corresponding tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sub\n",
            "ject\n",
            "Subject\n",
            ":\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(3323))\n",
        "print(tokenizer.decode(622))\n",
        "print(tokenizer.decode([3323, 622]))\n",
        "print(tokenizer.decode(29901))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using `pipeline`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJno8hX-93J8"
      },
      "source": [
        "When you use an LLM, two models are loaded:\n",
        "\n",
        "- The generative `model` itself\n",
        "- Its underlying `tokenizer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "b1b7bd5ec5bb4f1d9c0ee779c62f7c61",
            "cb5d733c70c04776af9d521e7bcc60e8",
            "2c43b1c41cc64c66bd7605e272d44201",
            "81fb635f1a854b919525ec85fb29bb61",
            "d33f5ff7a43e408899d1e7958fe7ecca",
            "4ae805a91e0141d4aede0a66a06d1197",
            "fcb2808fc64c45efae8e5a4706fca817",
            "47637d5b9a794137897d1f1fc081b21f",
            "794a791a933840a7866bc11f830ef485",
            "0dcc531d679f4609926fd63b39374ab5",
            "0dfccda5a4524427a937924f275fd8b7"
          ]
        },
        "id": "0ACMDYV_9vuk",
        "outputId": "0fb13e77-3080-408c-d846-3b6c520148a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1b7bd5ec5bb4f1d9c0ee779c62f7c61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFnAV4w8OB4I"
      },
      "source": [
        "Note: having `device_map=\"cuda\"` assumes NVIDIA GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rY5-XRx-Vrd"
      },
      "source": [
        "Although we now have enough to start generating text, there is a nice trick in transformers that simplifies the process, namely `transformers.pipeline`. It encapsulates the model, tokenizer, and text generation process into a single function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izzy6WkS9xJX"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPuiGlB7-aLN"
      },
      "source": [
        "The following parameters are worth mentioning:\n",
        "\n",
        "- `return_full_text`\n",
        "By setting this to False, the prompt will not be returned but merely the output of the model.\n",
        "\n",
        "- `max_new_tokens`\n",
        "The maximum number of tokens the model will generate. By setting a limit, we prevent long and unwieldy output as some models might continue generating output until they reach their context window.\n",
        "\n",
        "- `do_sample`\n",
        "Whether the model uses a sampling strategy to choose the next token. By setting this to False, the model will always select the next most probable token. In Chapter 6, we explore several sampling parameters that invoke some creativity in the model’s output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utuNHEsy-kjp",
        "outputId": "527dafba-b817-4cfc-8071-5971a993f395"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Why did the chicken join the band? Because it had the drumsticks!\n"
          ]
        }
      ],
      "source": [
        "# The prompt (user input / query)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# Generate output\n",
        "output = generator(messages)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2miIagZFO5Tb",
        "outputId": "c98ed1a2-5f90-4113-83aa-2e3b1a43d3a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Create a funny joke about chickens.\n",
            "Output:  Why did the chicken join the band? Because it had the drumsticks!\n"
          ]
        }
      ],
      "source": [
        "print(f'Prompt: {messages[0][\"content\"]}')\n",
        "print(f'Output: {output[0][\"generated_text\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvDoa8QZShlS"
      },
      "source": [
        "Let's make it into a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvlHB5EIShDV",
        "outputId": "6db14e5e-de45-4499-a087-4e8729122a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Why is the desert so hot? short answer:\n",
            "Generated Output:  The desert is hot due to consistent high solar radiation and minimal cloud cover, which allows for more direct and intense sunlight throughout the day. Low humidity also contributes to efficient heat absorption as there's less moisture in the air to retain the heat.\n"
          ]
        }
      ],
      "source": [
        "def generate_text(prompt):\n",
        "  messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "  output = generator(messages)\n",
        "  print(f\"Prompt: {prompt}\")\n",
        "  print(f\"Generated Output: {output[0]['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_RjPcJyTT-I",
        "outputId": "28abd682-1a83-4f66-adf5-27f128d9883b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: what is the sentiment of the following review? 'I love this product! It's amazing.'\n",
            "Generated Output:  The sentiment of the given review, 'I love this product! It's amazing,' is positive. The reviewer expresses strong positive feelings by using words like \"love\" and \"amazing,\" indicating a high level of satisfaction with the product.\n"
          ]
        }
      ],
      "source": [
        "generate_text(\"what is the sentiment of the following review? 'I love this product! It's amazing.'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nOWgvLCTm-x",
        "outputId": "72fe26c8-6c3c-493a-9271-57cd46d6b566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: what is the sentiment of the following review? 'I love this product! It's amazing.' answer with one word, either: POSITIVE or NEGATIVE\n",
            "Generated Output:  POSITIVE\n"
          ]
        }
      ],
      "source": [
        "generate_text(\"what is the sentiment of the following review? 'I love this product! It's amazing.' answer with one word, either: POSITIVE or NEGATIVE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKQAkSeuTcj7",
        "outputId": "6047d5cc-e126-4b60-b66e-afc2d334ad7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: how do you write Adam backwards?\n",
            "Generated Output:  To write \"Adam\" backward, you would reverse the order of its letters. So, the reversed version of \"Adam\" would be \"madA\".\n"
          ]
        }
      ],
      "source": [
        "generate_text(\"how do you write Adam backwards?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkKuMncNOpyS"
      },
      "source": [
        "### Your turn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-jL4OOMOplL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MoqnKDUN2WH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yR-MJTHN2Ug"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRkXm4agN2SA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIfBt4PyN2Jv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q38ZQVXcN2Ho"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JF3ZyKxN2rh"
      },
      "source": [
        "# Other Architectural Experiments and Improvements for Transformers\n",
        "\n",
        "Many tweaks of the Transformer are proposed and researched on a continuous basis. [“A Survey of Transformers”](https://oreil.ly/3SrG4) highlights a few of the main directions.\n",
        "\n",
        "Transformer architectures are also constantly adapted to domains beyond LLMs:\n",
        "\n",
        "- see: [“Transformers in vision: A survey”](https://oreil.ly/35CES) and [“A survey on vision transformer”](https://oreil.ly/0zEbq)\n",
        "- see [“Open X-Embodiment: Robotic learning datasets and RT-X models”](https://oreil.ly/SXAuB)\n",
        "- see [“Transformers in time series: A survey”](https://oreil.ly/p9duV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0acIFuLbO1qC"
      },
      "source": [
        "# Practical Tips: on Model Selection\n",
        "\n",
        "Choosing the right models is not as straightforward as you might think with over 60,000 models on the Hugging Face Hub for text classification and more than 8,000 models that generate embeddings at the moment of writing.\n",
        "\n",
        "### Start simple\n",
        "\n",
        "> It is highly advised to compare against classic, but strong baselines such as representing text with **TF-IDF** and training a logistic regression classifier on top of that. -- the authors of Hands-on LLMs.\n",
        "\n",
        "### 1. LLM for Representation vs. Generation\n",
        "\n",
        "#### 1.1 Representation models\n",
        "\n",
        "- Encoder (e.g., BERT, legal-BERT).\n",
        "\n",
        "While generative models, like the GPT family, are incredible models, encoder-only models similarly:\n",
        "- excel in task-specific use cases and\n",
        "- tend to be significantly smaller in size\n",
        "\n",
        "Selecting the right model for the job can be a form of art in itself. However, consider these as solid baselines:\n",
        "\n",
        "- BERT base model (uncased)\n",
        "- RoBERTa base model\n",
        "- DistilBERT base model (uncased)\n",
        "- DeBERTa base model\n",
        "- bert-tiny\n",
        "- ALBERT base v2\n",
        "\n",
        "The [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) is a great place to start looking for **embedding models**.\n",
        "- Contains open and closed source models\n",
        "- Benchmarked across several tasks:\n",
        "    - Classification is the task of assigning a label to a text.\n",
        "    - Clustering is the task of grouping similar documents together.\n",
        "    - Retrieval is the task of finding relevant documents for a query.\n",
        "    - ..etc.\n",
        "\n",
        "#### 1.2 Generation models\n",
        "\n",
        "- Decoder (e.g., GPT-style models)\n",
        "- Encoder-decoder (e.g., T5, BART)\n",
        "\n",
        "Used in tasks like:\n",
        "- abstractive summarization\n",
        "- translation\n",
        "- conversations\n",
        "\n",
        "### 2. Domain\n",
        "\n",
        "The closer to your target-domain, the better.\n",
        "\n",
        "**Experiment**\n",
        "\n",
        "Try the [Twitter-RoBERTa-base for Sentiment Analysis](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) model (trained on tweets) and use it for a movie reviews dataset ([`rotten_tomatoes`](https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes)) and compare it to [DistilBERT base uncased finetuned SST-2](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english) (trained on moview reviews).\n",
        "\n",
        "Other examples:\n",
        "- [`legal-bert`](https://huggingface.co/nlpaueb/legal-bert-base-uncased) an embedding model trained on Legal Documents.\n",
        "- [`biobert`](https://huggingface.co/dmis-lab/biobert-base-cased-v1.2) an embedding model trained on Medical Documents.\n",
        "\n",
        "### 3. Language\n",
        "\n",
        "English vs Arabic or Multi-lingual, or ...?\n",
        "\n",
        "This affects:\n",
        "\n",
        "- Size of pre-training data\n",
        "    - perhaps we need to start from scratch if it is very specialized domain like medical (if no data)\n",
        "- Availability of supervised fine-tuning data\n",
        "    - perhaps we need to start annotating a new dataset (if no data)\n",
        "\n",
        "### 4. Performance\n",
        "\n",
        "#### 4.1 Inference Speed\n",
        "\n",
        "The importance of inference speed should not be underestimated in real-life solutions. As such, we will use [`sentence-transformers/all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) as the embedding throughout this section. It is a small but performant model.\n",
        "\n",
        "\n",
        "#### 4.2 Inference Correctness\n",
        "\n",
        "As evaluated by metrics.\n",
        "\n",
        "### 5. Computation Requirements\n",
        "\n",
        "Model size or VRAM required.\n",
        "\n",
        "Can you run the model on your hardware or do you need cloud resources?\n",
        "\n",
        "### 6. Modality\n",
        "\n",
        "- text-only (BERT, GPT)\n",
        "- vision (ViT)\n",
        "- text + vision (VLLM)\n",
        "- text + voice"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dcc531d679f4609926fd63b39374ab5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dfccda5a4524427a937924f275fd8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c43b1c41cc64c66bd7605e272d44201": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47637d5b9a794137897d1f1fc081b21f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_794a791a933840a7866bc11f830ef485",
            "value": 2
          }
        },
        "47637d5b9a794137897d1f1fc081b21f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ae805a91e0141d4aede0a66a06d1197": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "794a791a933840a7866bc11f830ef485": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81fb635f1a854b919525ec85fb29bb61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dcc531d679f4609926fd63b39374ab5",
            "placeholder": "​",
            "style": "IPY_MODEL_0dfccda5a4524427a937924f275fd8b7",
            "value": " 2/2 [00:33&lt;00:00, 15.82s/it]"
          }
        },
        "b1b7bd5ec5bb4f1d9c0ee779c62f7c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb5d733c70c04776af9d521e7bcc60e8",
              "IPY_MODEL_2c43b1c41cc64c66bd7605e272d44201",
              "IPY_MODEL_81fb635f1a854b919525ec85fb29bb61"
            ],
            "layout": "IPY_MODEL_d33f5ff7a43e408899d1e7958fe7ecca"
          }
        },
        "cb5d733c70c04776af9d521e7bcc60e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ae805a91e0141d4aede0a66a06d1197",
            "placeholder": "​",
            "style": "IPY_MODEL_fcb2808fc64c45efae8e5a4706fca817",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d33f5ff7a43e408899d1e7958fe7ecca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcb2808fc64c45efae8e5a4706fca817": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
